<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ConcaXu</title>
  
  
  <link href="https://username.github.io/project/atom.xml" rel="self"/>
  
  <link href="https://username.github.io/project/"/>
  <updated>2024-07-18T06:48:51.664Z</updated>
  <id>https://username.github.io/project/</id>
  
  <author>
    <name>Conca Xu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>YoloBaseNode</title>
    <link href="https://username.github.io/project/2024/07/18/YoloBaseNode/"/>
    <id>https://username.github.io/project/2024/07/18/YoloBaseNode/</id>
    <published>2024-07-18T06:38:36.000Z</published>
    <updated>2024-07-18T06:48:51.664Z</updated>
    
    <content type="html"><![CDATA[<p>​<br>目录</p><h3 id="Yolo的任务分类"><a href="#Yolo的任务分类" class="headerlink" title="Yolo的任务分类"></a>Yolo的任务分类</h3><h3 id="Yolo的模式类型"><a href="#Yolo的模式类型" class="headerlink" title="Yolo的模式类型"></a>Yolo的模式类型</h3><h3 id="Yolo的任务分类-1"><a href="#Yolo的任务分类-1" class="headerlink" title="Yolo的任务分类"></a>Yolo的任务分类</h3><h3 id="Train（训练）"><a href="#Train（训练）" class="headerlink" title="Train（训练）"></a>Train（训练）</h3><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p>demo<br>from ultralytics import YOLO<br>​</p><h1 id="Load-a-model"><a href="#Load-a-model" class="headerlink" title="Load a model"></a>Load a model</h1><p>model &#x3D; YOLO(“yolov8n.yaml”)  # build a new model from YAML<br>model &#x3D; YOLO(“yolov8n.pt”)  # load a pretrained model (recommended for training)<br>model &#x3D; YOLO(“yolov8n.yaml”).load(“yolov8n.pt”)  # build from YAML and transfer weights<br>​</p><h1 id="Train-the-model"><a href="#Train-the-model" class="headerlink" title="Train the model"></a>Train the model</h1><p>results &#x3D; model.train(data&#x3D;”coco8.yaml”, epochs&#x3D;100, imgsz&#x3D;640)<br>参数<br>参数默认值说明<br>modelNone指定用于训练的模型文件。接受指向 .pt 预训练模型或 .yaml 配置文件。对于定义模型结构或初始化权重至关重要。<br>dataNone数据集配置文件的路径（例如 coco8.yaml).该文件包含特定于数据集的参数，包括训练数据和验证数据的路径、类名和类数。<br>epochs100训练历元总数。每个历元代表对整个数据集进行一次完整的训练。调整该值会影响训练时间和模型性能。<br>timeNone最长训练时间（小时）。如果设置了该值，则会覆盖 epochs 参数，允许训练在指定的持续时间后自动停止。对于时间有限的训练场景非常有用。<br>patience100在验证指标没有改善的情况下，提前停止训练所需的历元数。当性能趋于平稳时停止训练，有助于防止过度拟合。<br>batch16训练的批量大小，表示在更新模型内部参数之前要处理多少张图像。自动批处理 (batch&#x3D;-1)会根据 GPU 内存可用性动态调整批处理大小。<br>imgsz640用于训练的目标图像尺寸。所有图像在输入模型前都会被调整到这一尺寸。影响模型精度和计算复杂度。<br>saveTrue可保存训练检查点和最终模型权重。这对恢复训练或模型部署非常有用。<br>save_period-1保存模型检查点的频率，以 epochs 为单位。值为-1 时将禁用此功能。该功能适用于在长时间训练过程中保存临时模型。<br>cacheFalse在内存中缓存数据集图像 (True&#x2F;ram）、磁盘 (disk），或禁用它 (False).通过减少磁盘 I&#x2F;O 提高训练速度，但代价是增加内存使用量。<br>deviceNone指定用于训练的计算设备：单个 GPU (device&#x3D;0）、多个 GPU (device&#x3D;0,1)、CPU (device&#x3D;cpu)，或苹果芯片的 MPS (device&#x3D;mps).<br>workers8加载数据的工作线程数（每 RANK 多 GPU 训练）。影响数据预处理和输入模型的速度，尤其适用于多 GPU 设置。<br>projectNone保存训练结果的项目目录名称。允许有组织地存储不同的实验。<br>nameNone训练运行的名称。用于在项目文件夹内创建一个子目录，用于存储训练日志和输出结果。<br>exist_okFalse如果为 True，则允许覆盖现有的项目&#x2F;名称目录。这对迭代实验非常有用，无需手动清除之前的输出。<br>pretrainedTrue决定是否从预处理模型开始训练。可以是布尔值，也可以是加载权重的特定模型的字符串路径。提高训练效率和模型性能。<br>optimizer‘auto’为培训选择优化器。选项包括 SGD, Adam, AdamW, NAdam, RAdam, RMSProp 等，或 auto 用于根据模型配置进行自动选择。影响收敛速度和稳定性<br>verboseFalse在训练过程中启用冗长输出，提供详细日志和进度更新。有助于调试和密切监控培训过程。<br>seed0为训练设置随机种子，确保在相同配置下运行的结果具有可重复性。<br>deterministicTrue强制使用确定性算法，确保可重复性，但由于对非确定性算法的限制，可能会影响性能和速度。<br>single_clsFalse在训练过程中将多类数据集中的所有类别视为单一类别。适用于二元分类任务，或侧重于对象的存在而非分类。<br>rectFalse可进行矩形训练，优化批次组成以减少填充。这可以提高效率和速度，但可能会影响模型的准确性。<br>cos_lrFalse利用余弦学习率调度器，根据历时的余弦曲线调整学习率。这有助于管理学习率，实现更好的收敛。<br>close_mosaic10在训练完成前禁用最后 N 个历元的马赛克数据增强以稳定训练。设置为 0 则禁用此功能。<br>resumeFalse从上次保存的检查点恢复训练。自动加载模型权重、优化器状态和历时计数，无缝继续训练。<br>ampTrue启用自动混合精度 (AMP) 训练，可减少内存使用量并加快训练速度，同时将对精度的影响降至最低。<br>fraction1.0指定用于训练的数据集的部分。允许在完整数据集的子集上进行训练，这对实验或资源有限的情况非常有用。<br>profileFalse在训练过程中，可对ONNX 和TensorRT 速度进行剖析，有助于优化模型部署。<br>freezeNone冻结模型的前 N 层或按索引指定的层，从而减少可训练参数的数量。这对微调或迁移学习非常有用。<br>lr00.01初始学习率（即 SGD&#x3D;1E-2, Adam&#x3D;1E-3) .调整这个值对优化过程至关重要，会影响模型权重的更新速度。<br>lrf0.01最终学习率占初始学习率的百分比 &#x3D; (lr0 * lrf)，与调度程序结合使用，随着时间的推移调整学习率。<br>momentum0.937用于 SGD 的动量因子，或用于 Adam 优化器的 beta1，用于将过去的梯度纳入当前更新。<br>weight_decay0.0005L2 正则化项，对大权重进行惩罚，以防止过度拟合。<br>warmup_epochs3.0学习率预热的历元数，学习率从低值逐渐增加到初始学习率，以在早期稳定训练。<br>warmup_momentum0.8热身阶段的初始动力，在热身期间逐渐调整到设定动力。<br>warmup_bias_lr0.1热身阶段的偏置参数学习率，有助于稳定初始历元的模型训练。<br>box7.5损失函数中边框损失部分的权重，影响对准确预测边框坐标的重视程度。<br>cls0.5分类损失在总损失函数中的权重，影响正确分类预测相对于其他部分的重要性。<br>dfl1.5分布焦点损失权重，在某些YOLO 版本中用于精细分类。<br>pose12.0姿态损失在姿态估计模型中的权重，影响着准确预测姿态关键点的重点。<br>kobj2.0姿态估计模型中关键点对象性损失的权重，平衡检测可信度与姿态精度。<br>label_smoothing0.0应用标签平滑，将硬标签软化为目标标签和标签均匀分布的混合标签，可以提高泛化效果。<br>nbs64用于损耗正常化的标称批量大小。<br>overlap_maskTrue决定在训练过程中分割掩码是否应该重叠，适用于实例分割任务。<br>mask_ratio4分割掩码的下采样率，影响训练时使用的掩码分辨率。<br>dropout0.0分类任务中正则化的丢弃率，通过在训练过程中随机省略单元来防止过拟合。<br>valTrue可在训练过程中进行验证，以便在单独的数据集上对模型性能进行定期评估。<br>plotsFalse生成并保存训练和验证指标图以及预测示例图，以便直观地了解模型性能和学习进度。<br>​</p><p>超参数<br>参数名类型默认值范围说明<br>hsv_hfloat0.0150.0 - 1.0通过色轮的一部分来调整图像的色调，从而引入色彩的可变性。帮助模型在不同的光照条件下通用。<br>hsv_sfloat0.70.0 - 1.0改变图像饱和度的一部分，影响色彩的强度。可用于模拟不同的环境条件。<br>hsv_vfloat0.40.0 - 1.0将图像的数值（亮度）修改一部分，帮助模型在不同的光照条件下表现良好。<br>degreesfloat0.0-180 - +180在指定的度数范围内随机旋转图像，提高模型识别不同方向物体的能力。<br>translatefloat0.10.0 - 1.0以图像大小的一小部分水平和垂直平移图像，帮助学习检测部分可见的物体。<br>scalefloat0.5&gt;&#x3D;0.0通过增益因子缩放图像，模拟物体与摄像机的不同距离。<br>shearfloat0.0-180 - +180按指定角度剪切图像，模拟从不同角度观察物体的效果。<br>perspectivefloat0.00.0 - 0.001对图像进行随机透视变换，增强模型理解三维空间中物体的能力。<br>flipudfloat0.00.0 - 1.0以指定的概率将图像翻转过来，在不影响物体特征的情况下增加数据的可变性。<br>fliplrfloat0.50.0 - 1.0以指定的概率将图像从左到右翻转，这对学习对称物体和增加数据集多样性非常有用。<br>bgrfloat0.00.0 - 1.0以指定的概率将图像通道从 RGB 翻转到 BGR，用于提高对错误通道排序的稳健性。<br>mosaicfloat1.00.0 - 1.0将四幅训练图像合成一幅，模拟不同的场景构成和物体互动。对复杂场景的理解非常有效。<br>mixupfloat0.00.0 - 1.0混合两幅图像及其标签，创建合成图像。通过引入标签噪声和视觉变化，增强模型的泛化能力。<br>copy_pastefloat0.00.0 - 1.0从一幅图像中复制物体并粘贴到另一幅图像上，用于增加物体实例和学习物体遮挡。<br>auto_augmentstrrandaugment-自动应用预定义的增强策略 (randaugment, autoaugment, augmix)，通过丰富视觉特征来优化分类任务。<br>erasingfloat0.40.0 - 0.9在分类训练过程中随机擦除部分图像，鼓励模型将识别重点放在不明显的特征上。<br>crop_fractionfloat1.00.1 - 1.0将分类图像裁剪为其大小的一小部分，以突出中心特征并适应对象比例，减少背景干扰。<br>​</p><p>常用参数<br>epoch、 imgsz、 save、 device、 workers<br>Val（评估）<br>概念</p><p>demo<br>from ultralytics import YOLO<br>​</p><h1 id="Load-a-model-1"><a href="#Load-a-model-1" class="headerlink" title="Load a model"></a>Load a model</h1><p>model &#x3D; YOLO(“yolov8n.pt”)  # load an official model<br>model &#x3D; YOLO(“path&#x2F;to&#x2F;best.pt”)  # load a custom model<br>​</p><h1 id="Validate-the-model"><a href="#Validate-the-model" class="headerlink" title="Validate the model"></a>Validate the model</h1><p>metrics &#x3D; model.val()  # no arguments needed, dataset and settings remembered<br>metrics.box.map  # map50-95<br>metrics.box.map50  # map50<br>metrics.box.map75  # map75<br>metrics.box.maps  # a list contains map50-95 of each category<br>参数&#x2F;超参数<br>参数类型默认值说明<br>datastrNone指定数据集配置文件的路径（如 coco8.yaml).该文件包括验证数据的路径、类名和类数。<br>imgszint640定义输入图像的尺寸。所有图像在处理前都会调整到这一尺寸。<br>batchint16设置每批图像的数量。使用 -1 的自动批处理功能，可根据 GPU 内存可用性自动调整。<br>save_jsonboolFalse如果 True此外，还可将结果保存到 JSON 文件中，以便进一步分析或与其他工具集成。<br>save_hybridboolFalse如果 True，保存混合版本的标签，将原始注释与额外的模型预测相结合。<br>conffloat0.001设置检测的最小置信度阈值。置信度低于此阈值的检测将被丢弃。<br>ioufloat0.6设置非最大抑制 (NMS) 的交叉重叠 (IoU) 阈值。有助于减少重复检测。<br>max_detint300限制每幅图像的最大检测次数。在密度较高的场景中非常有用，可以防止检测次数过多。<br>halfboolTrue可进行半精度（FP16）计算，减少内存使用量，在提高速度的同时，将对精度的影响降至最低。<br>devicestrNone指定验证设备 (cpu, cuda:0等）。可灵活利用 CPU 或 GPU 资源。<br>dnnboolFalse如果 True它使用 OpenCV DNN 模块进行ONNX 模型推断，为PyTorch 推断方法提供了一种替代方法。<br>plotsboolFalse当设置为 True此外，它还能生成并保存预测结果与地面实况的对比图，以便对模型的性能进行可视化评估。<br>rectboolFalse如果 True该软件使用矩形推理进行批处理，减少了填充，可能会提高速度和效率。<br>splitstrval确定用于验证的数据集分割 (val, test或 train).可灵活选择数据段进行性能评估。<br>​</p><p>常用参数<br>batch 、 conf、 IoU、 half<br>Predict(预测)<br>demo<br>from ultralytics import YOLO<br>​</p><h1 id="Load-a-pretrained-YOLOv8n-model"><a href="#Load-a-pretrained-YOLOv8n-model" class="headerlink" title="Load a pretrained YOLOv8n model"></a>Load a pretrained YOLOv8n model</h1><p>model &#x3D; YOLO(“yolov8n.pt”)<br>​</p><h1 id="Run-inference-on-‘bus-jpg’-with-arguments"><a href="#Run-inference-on-‘bus-jpg’-with-arguments" class="headerlink" title="Run inference on ‘bus.jpg’ with arguments"></a>Run inference on ‘bus.jpg’ with arguments</h1><p>result &#x3D; model.predict(“bus.jpg”, save&#x3D;True, imgsz&#x3D;320, conf&#x3D;0.5)</p><h1 id="Value-of-result"><a href="#Value-of-result" class="headerlink" title="Value of result"></a>Value of result</h1><p>for result in results:<br>    print(“图像原始大小:”, result.orig_shape)<br>    data_numpy &#x3D; result.boxes.data.cpu().numpy()<br>    for data in data_numpy:<br>        print(“data”, data)<br>        print(“左上角X轴坐标:”, data[0])<br>        print(“左上角Y轴坐标:”, data[1])<br>        print(“右下角X轴坐标:”, data[2])<br>        print(“右下角Y轴坐标:”, data[3])<br>        print(“置—信—度:”, data[4])<br>        print(“检测到的类有:”,  data[5])</p><p>参数<br>论据类型默认值说明<br>sourcestr‘ultralytics&#x2F;assets’指定推理的数据源。可以是图像路径、视频文件、目录、URL 或用于实时馈送的设备 ID。支持多种格式和来源，可灵活应用于不同类型的输入。<br>conffloat0.25设置检测的最小置信度阈值。如果检测到的对象置信度低于此阈值，则将不予考虑。调整该值有助于减少误报。<br>ioufloat0.7非最大抑制 (NMS) 的交叉重叠 (IoU) 阈值。较低的数值可以消除重叠的方框，从而减少检测次数，这对减少重复检测非常有用。<br>imgszint or tuple640定义用于推理的图像大小。可以是一个整数 640 或一个（高、宽）元组。适当调整大小可以提高检测精度和处理速度。<br>halfboolFalse启用半精度（FP16）推理，可加快支持的 GPU 上的模型推理速度，同时将对精度的影响降至最低。<br>devicestrNone指定用于推理的设备（例如：……）、 cpu, cuda:0 或 0).允许用户选择 CPU、特定 GPU 或其他计算设备来执行模型。<br>max_detint300每幅图像允许的最大检测次数。限制模型在单次推理中可检测到的物体总数，防止在密集场景中产生过多输出。<br>vid_strideint1视频输入的帧间距。允许跳过视频中的帧，以加快处理速度，但会牺牲时间分辨率。值为 1 时处理每一帧，值越大跳帧越多。<br>stream_bufferboolFalse确定在处理视频流时是否对所有帧进行缓冲 (True)，或者模型是否应该返回最近的帧 (False).适用于实时应用。<br>visualizeboolFalse在推理过程中激活模型特征的可视化，从而深入了解模型 “看到 “了什么。这对调试和模型解释非常有用。<br>augmentboolFalse可对预测进行测试时间增强（TTA），从而在牺牲推理速度的情况下提高检测的鲁棒性。<br>agnostic_nmsboolFalse启用与类别无关的非最大抑制 (NMS)，可合并不同类别的重叠方框。这在多类检测场景中非常有用，因为在这种场景中，类的重叠很常见。<br>classeslist[int]None根据一组类别 ID 过滤预测结果。只返回属于指定类别的检测结果。在多类检测任务中，该功能有助于集中检测相关对象。<br>retina_masksboolFalse如果模型中存在高分辨率的分割掩膜，则使用高分辨率的分割掩膜。这可以提高分割任务的掩膜质量，提供更精细的细节。<br>embedlist[int]None指定从中提取特征向量或嵌入的层。这对聚类或相似性搜索等下游任务非常有用。<br>​</p><p>常用参数<br>conf、 IoU、 half、 imgsz、 device、 classes、<br>结果参数<br>属性类型说明<br>orig_imgnumpy.ndarray原始图像的 numpy 数组。<br>orig_shapetuple原始图像的形状，格式为（高、宽）。<br>boxesBoxes, optional包含检测边界框的方框对象。<br>masksMasks, optional包含检测掩码的掩码对象。<br>probsProbs, optionalProbs 对象，包含分类任务中每个类别的概率。<br>keypointsKeypoints, optional关键点对象，包含每个对象的检测关键点。<br>obbOBB, optional包含定向包围盒的 OBB 对象。<br>speeddict每幅图像的预处理、推理和后处理速度字典，单位为毫秒。<br>namesdict类名字典。<br>pathstr图像文件的路径。<br>​</p><p>results支持方法<br>方法返回类型说明<br>update()None更新结果对象的方框、掩码和 probs 属性。<br>cpu()Results返回包含 CPU 内存中所有张量的结果对象副本。<br>numpy()Results返回结果对象的副本，其中所有张量均为 numpy 数组。<br>cuda()Results返回包含 GPU 内存中所有张量的 Results 对象副本。<br>to()Results返回带有指定设备和 dtype 上张量的 Results 对象副本。<br>new()Results返回一个具有相同图像、路径和名称的新结果对象。<br>plot()numpy.ndarray绘制检测结果。返回注释图像的 numpy 数组。<br>show()None在屏幕上显示带注释的结果。<br>save()None将注释结果保存到文件中。<br>verbose()str返回每个任务的日志字符串。<br>save_txt()None将预测结果保存到 txt 文件中。<br>save_crop()None将裁剪后的预测保存到 save_dir&#x2F;cls&#x2F;file_name.jpg.<br>tojson()str将对象转换为 JSON 格式。<br>​</p><p>Export（导出）</p><p>demo<br>from ultralytics import YOLO<br>​</p><h1 id="Load-a-model-2"><a href="#Load-a-model-2" class="headerlink" title="Load a model"></a>Load a model</h1><p>model &#x3D; YOLO(“yolov8n.pt”)  # load an official model<br>model &#x3D; YOLO(“path&#x2F;to&#x2F;best.pt”)  # load a custom trained model<br>​</p><h1 id="Export-the-model"><a href="#Export-the-model" class="headerlink" title="Export the model"></a>Export the model</h1><p>model.export(format&#x3D;”onnx”)<br>参数<br>论据类型默认值说明<br>formatstr‘torchscript’导出模型的目标格式，例如 ‘onnx’, ‘torchscript’, ‘tensorflow’或其他，定义与各种部署环境的兼容性。<br>imgszint 或 tuple640模型输入所需的图像尺寸。对于正方形图像，可以是一个整数，或者是一个元组 (height, width) 了解具体尺寸。<br>kerasboolFalse启用导出为 Keras 格式的TensorFlow SavedModel ，提供与TensorFlow serving 和 API 的兼容性。<br>optimizeboolFalse在导出到TorchScript 时，应用针对移动设备的优化，可能会减小模型大小并提高性能。<br>halfboolFalse启用 FP16（半精度）量化，在支持的硬件上减小模型大小并可能加快推理速度。<br>int8boolFalse激活 INT8 量化，进一步压缩模型并加快推理速度，同时将精度损失降至最低，主要用于边缘设备。<br>dynamicboolFalse允许ONNX 和TensorRT 导出动态输入尺寸，提高了处理不同图像尺寸的灵活性。<br>simplifyboolFalse简化了ONNX 导出的模型图，可能会提高性能和兼容性。<br>opsetintNone指定ONNX opset 版本，以便与不同的ONNX 解析器和运行时兼容。如果未设置，则使用最新的支持版本。<br>workspacefloat4.0为TensorRT 优化设置最大工作区大小（GiB），以平衡内存使用和性能。<br>nmsboolFalse在CoreML 导出中添加非最大值抑制 (NMS)，这对精确高效的检测后处理至关重要。<br>batchint1指定导出模型的批量推理大小，或导出模型将同时处理的图像的最大数量。 predict 模式。<br>​</p><p>支持导出格式<br>​</p><p>格式format 论据模型元数据论据<br>PyTorch-yolov8n.pt✅-<br>TorchScripttorchscriptyolov8n.torchscript✅imgsz, optimize, batch<br>ONNXonnxyolov8n.onnx✅imgsz, half, dynamic, simplify, opset, batch<br>OpenVINOopenvinoyolov8n_openvino_model&#x2F;✅imgsz, half, int8, batch<br>TensorRTengineyolov8n.engine✅imgsz, half, dynamic, simplify, workspace, int8, batch<br>CoreMLcoremlyolov8n.mlpackage✅imgsz, half, int8, nms, batch<br>TF SavedModelsaved_modelyolov8n_saved_model&#x2F;✅imgsz, keras, int8, batch<br>TF GraphDefpbyolov8n.pb❌imgsz, batch<br>TF 轻型tfliteyolov8n.tflite✅imgsz, half, int8, batch<br>TF 边缘TPUedgetpuyolov8n_edgetpu.tflite✅imgsz, batch<br>TF.jstfjsyolov8n_web_model&#x2F;✅imgsz, half, int8, batch<br>PaddlePaddlepaddleyolov8n_paddle_model&#x2F;✅imgsz, batch<br>NCNNncnnyolov8n_ncnn_model&#x2F;✅imgsz, half, batch<br>​</p><p>Yolo的模式类型<br>Detect（检测）</p><p>模型<br>​</p><p>模型尺寸<br>（像素）mAPval<br>50-95速度<br>CPUONNX<br>(ms)速度<br>A100 TensorRT<br>（毫秒）params<br>(M)FLOPs<br>(B)<br>YOLOv8n64037.380.40.993.28.7<br>YOLOv8s64044.9128.41.2011.228.6<br>YOLOv8m64050.2234.71.8325.978.9<br>YOLOv8l64052.9375.22.3943.7165.2<br>YOLOv8x64053.9479.13.5368.2257.8<br>​</p><p>Segment（分割）</p><p>模型<br>​</p><p>模型尺寸<br>（像素）mAPbox<br>50-95mAPmask<br>50-95速度<br>CPUONNX<br>(ms)速度<br>A100 TensorRT<br>（毫秒）params<br>(M)FLOPs<br>(B)<br>YOLOv8n-seg64036.730.596.11.213.412.6<br>YOLOv8s-seg64044.636.8155.71.4711.842.6<br>YOLOv8m-seg64049.940.8317.02.1827.3110.2<br>YOLOv8l-seg64052.342.6572.42.7946.0220.5<br>YOLOv8x-seg64053.443.4712.14.0271.8344.1<br>​</p><p>Classify（分类）</p><p>模型<br>​</p><p>模型尺寸<br>（像素）acc<br>top1acc<br>top5速度<br>CPUONNX<br>(ms)速度<br>A100 TensorRT<br>（毫秒）params<br>(M)FLOPs<br>(B) at 640<br>YOLOv8n-cls22469.088.312.90.312.74.3<br>YOLOv8s-cls22473.891.723.40.356.413.5<br>YOLOv8m-cls22476.893.585.40.6217.042.7<br>YOLOv8l-cls22476.893.5163.00.8737.599.7<br>YOLOv8x-cls22479.094.6232.01.0157.4154.8<br>​</p><p>Pose（姿态）</p><p>模型<br>​</p><p>模型尺寸<br>（像素）<br>50-95mAPpose<br>50速度<br>CPUONNX<br>(ms)速度<br>A100 TensorRT<br>（毫秒）params<br>(M)FLOPs<br>(B)<br>YOLOv8n-姿势64050.480.1131.81.183.39.2<br>YOLOv8s-姿势64060.086.2233.21.4211.630.2<br>YOLOv8m-姿势64065.088.8456.32.0026.481.0<br>YOLOv8l-姿势64067.690.0784.52.5944.4168.6<br>YOLOv8x-姿势64069.290.21607.13.7369.4263.2<br>YOLOv8x-pose-p6128071.691.24088.710.0499.11066.4<br>​</p><p>OBB（定向检测）</p><p>模型<br>模型尺寸<br>（像素）mAPtest<br>50速度<br>CPUONNX<br>(ms)速度<br>A100 TensorRT<br>（毫秒）params<br>(M)FLOPs<br>(B)<br>YOLOv8n-obb102478.0204.773.573.123.3<br>YOLOv8s-obb102479.5424.884.0711.476.3<br>YOLOv8m-obb102480.5763.487.6126.4208.6<br>YOLOv8l-obb102480.71278.4211.8344.5433.8<br>YOLOv8x-obb102481.361759.1013.2369.5676.7<br>​</p><p>个人经验<br>如何使用Yolov8</p><p>pip install ultralytics<br>pip3 install torch torchvision torchaudio –index-url <a href="https://download.pytorch.org/whl/cu118">https://download.pytorch.org/whl/cu118</a> (根据自己电脑配置决定)</p><p>nvidia-smi 查看自己电脑支持的最高cuda版本，建议安装11.8比较稳定<br>然后安装对应支持的cuda，cudnn<br>参考：<a href="https://blog.csdn.net/jhsignal/article/details/111401628">https://blog.csdn.net/jhsignal/article/details/111401628</a></p><p>import torch<br>print(torch.cuda.is_available())<br>x &#x3D; torch.rand(5, 5).cuda()<br>print(x)<br>训练结果参数理解</p><p>训练自己的数据集</p><p>path: D:\PythonProject\ultralytics\datasets\myselfData # dataset中的文件夹路径<br>train: images&#x2F;train2017 # train images 训练的图片路径<br>val: images&#x2F;train2017 # val images 验证的图片路径<br>test: images&#x2F;train2017 # test images 测试的图片路径</p><h1 id="Classes"><a href="#Classes" class="headerlink" title="Classes"></a>Classes</h1><p>names:<br>0: Bread<br>1: Paper<br>2: People<br>启动训练函数<br>from ultralytics import YOLO</p><p>def train():</p><h1 id="确保配置文件是正确的，通常是与预训练模型相同的配置文件"><a href="#确保配置文件是正确的，通常是与预训练模型相同的配置文件" class="headerlink" title="确保配置文件是正确的，通常是与预训练模型相同的配置文件"></a>确保配置文件是正确的，通常是与预训练模型相同的配置文件</h1><p>model &#x3D; YOLO(‘yolov8m.yaml’).load(‘yolov8n.pt’)</p><pre><code># 开始训练模型results = model.train(data=&#39;datasets/myselfData/myselfData.yaml&#39;, epochs=300, batch=16, device=0, workers=8, imgsz=640) return results</code></pre><p>if <strong>name</strong> &#x3D;&#x3D; ‘<strong>main</strong>‘:<br>results &#x3D; train()<br>预测函数<br>import os</p><p>import cv2<br>import numpy as np<br>from ultralytics import YOLO</p><p>model &#x3D; YOLO(‘..&#x2F;yolopt&#x2F;best.pt’)<br>folder_path &#x3D; “D:&#x2F;PythonProject&#x2F;xxx” #可以使用绝对路径&#x2F;相对路径，文件夹图片都可以放<br>results &#x3D; model.predict(folder_path, save&#x3D;True, imgsz&#x3D;320, conf&#x3D;0.5, device&#x3D;’0’, half&#x3D;True)<br>for result in results:<br>data_numpy &#x3D; result.boxes.data.cpu().numpy()<br>for data in data_numpy:<br>print(“左上角X轴坐标:”, data[0])<br>print(“左上角Y轴坐标:”, data[1])<br>print(“右下角X轴坐标:”, data[2])<br>print(“右下角Y轴坐标:”, data[3])<br>print(“置—信—度:”, data[4])<br>print(“检测到的类有:”, [int(data[5])])<br>验证函数<br>from ultralytics import YOLO</p><p>def val():<br>model &#x3D; YOLO(“..&#x2F;bestX.pt”)<br>validation_results &#x3D; model.val(data&#x3D;”datasets&#x2F;myselfData&#x2F;myselfData.yaml”, imgsz&#x3D;640,<br>batch&#x3D;64, conf&#x3D;0.5, iou&#x3D;0.6, device&#x3D;”0”)</p><p>if <strong>name</strong> &#x3D;&#x3D; ‘<strong>main</strong>‘:<br>val()<br>模型转换（Pt转Onnx）<br>import cv2<br>import torch</p><p>from tensor_rt.main import YOLOv8<br>from ultralytics import YOLO</p><h1 id="Pt转换Onnx模型"><a href="#Pt转换Onnx模型" class="headerlink" title="# Pt转换Onnx模型"></a># Pt转换Onnx模型</h1><p>device &#x3D; “cuda:0” if torch.cuda.is_available() else “cpu”<br>onnx_model &#x3D; YOLO(“DarkPointX.onnx”)<br>results &#x3D; onnx_model(“202404201048220.jpg”)<br>print(results)<br> 模型（Pt转Engine）<br>from ultralytics import YOLO<br>import torch</p><h1 id="检查GPU是否可用，并设置为默认设备"><a href="#检查GPU是否可用，并设置为默认设备" class="headerlink" title="检查GPU是否可用，并设置为默认设备"></a>检查GPU是否可用，并设置为默认设备</h1><p>device &#x3D; ‘cuda’ if torch.cuda.is_available() else ‘cpu’<br>print(f”Using device: {device}”)</p><h1 id="载入模型并转移到设备"><a href="#载入模型并转移到设备" class="headerlink" title="载入模型并转移到设备"></a>载入模型并转移到设备</h1><p>model &#x3D; YOLO(“DarkPointX.pt”).to(device)<br>model.export(<br>format&#x3D;”engine”,<br>dynamic&#x3D;True,<br>batch&#x3D;32,<br>workspace&#x3D;4,<br>half&#x3D;True,<br>data&#x3D;”bread.yaml”,<br>)<br>print(“Done”)</p><h1 id="results-self-model-predict-image-save-False-imgsz-640-conf-self-conf-device-’0’-half-True"><a href="#results-self-model-predict-image-save-False-imgsz-640-conf-self-conf-device-’0’-half-True" class="headerlink" title="results &#x3D; self.model.predict(image, save&#x3D;False, imgsz&#x3D;640, conf&#x3D;self.conf, device&#x3D;’0’, half&#x3D;True)"></a>results &#x3D; self.model.predict(image, save&#x3D;False, imgsz&#x3D;640, conf&#x3D;self.conf, device&#x3D;’0’, half&#x3D;True)</h1><h1 id="Load-the-exported-TensorRT-INT8-model"><a href="#Load-the-exported-TensorRT-INT8-model" class="headerlink" title="Load the exported TensorRT INT8 model"></a>Load the exported TensorRT INT8 model</h1><p>model &#x3D; YOLO(“DarkPointX.engine”)</p><h1 id="Run-inference"><a href="#Run-inference" class="headerlink" title="Run inference"></a>Run inference</h1><p>result &#x3D; model.predict(“<a href="https://ultralytics.com/images/bus.jpg">https://ultralytics.com/images/bus.jpg</a>“)</p><p>​</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;​&lt;br&gt;目录&lt;/p&gt;
&lt;h3 id=&quot;Yolo的任务分类&quot;&gt;&lt;a href=&quot;#Yolo的任务分类&quot; class=&quot;headerlink&quot; title=&quot;Yolo的任务分类&quot;&gt;&lt;/a&gt;Yolo的任务分类&lt;/h3&gt;&lt;h3 id=&quot;Yolo的模式类型&quot;&gt;&lt;a href=&quot;#Y</summary>
      
    
    
    
    <category term="学习笔记" scheme="https://username.github.io/project/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="node.js" scheme="https://username.github.io/project/tags/node-js/"/>
    
    <category term="hexo" scheme="https://username.github.io/project/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://username.github.io/project/2024/04/07/hello-world/"/>
    <id>https://username.github.io/project/2024/04/07/hello-world/</id>
    <published>2024-04-06T16:38:36.000Z</published>
    <updated>2024-07-18T03:08:10.312Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    <category term="学习笔记" scheme="https://username.github.io/project/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="node.js" scheme="https://username.github.io/project/tags/node-js/"/>
    
    <category term="hexo" scheme="https://username.github.io/project/tags/hexo/"/>
    
  </entry>
  
</feed>
